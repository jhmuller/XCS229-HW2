% This contents of this file will be inserted into the _Solutions version of the
% output tex document.  Here's an example:

% If assignment with subquestion (1.a) requires a written response, you will
% find the following flag within this document: <SCPD_SUBMISSION_TAG>_1a
% In this example, you would insert the LaTeX for your solution to (1.a) between
% the <SCPD_SUBMISSION_TAG>_1a flags.  If you also constrain your answer between the
% START_CODE_HERE and END_CODE_HERE flags, your LaTeX will be styled as a
% solution within the final document.

% Please do not use the '<SCPD_SUBMISSION_TAG>' character anywhere within your code.  As expected,
% that will confuse the regular expressions we use to identify your solution.
\def\assignmentnum{2 }
\def\assignmenttitle{XCS229 Problem Set \assignmentnum}
\input{macros}
\begin{document}
\pagestyle{myheadings} \markboth{}{\assignmenttitle}

% <SCPD_SUBMISSION_TAG>_entire_submission

This handout includes space for every question that requires a written response.
Please feel free to use it to handwrite your solutions (legibly, please).  If
you choose to typeset your solutions, the |README.md| for this assignment includes
instructions to regenerate this handout with your typeset \LaTeX{} solutions.
\ruleskip

\LARGE
1.a
\normalsize

% <SCPD_SUBMISSION_TAG>_1a
\begin{answer}
  Since \\
  \begin{center}
  $g'(z) = g(z)(1-g(z))$ and \\
  $h(x) = g(\theta^T x)$, 
  \end{center} 
  it follows that 
  \begin{center}
   $\partial h(x) / \partial \theta_k = h(x)(1 - h(x)) x_k$.
  \end{center}
  Letting 
  \begin{center}
  $h_{\theta}(x^{(i)}) = g(\theta^T x^{(i)})
  = 1/(1 + \exp(-\theta^T x^{(i)}))$, 
  \end{center}
  we have\\

  \begin{flalign*}
    \frac{\partial\log h_{\theta}(x^{(i)})}{\partial\theta_k} &= 
    \frac{1}{h(\theta^T x)} \times  h(x)(1 - h(x)) x_k\\
    &=      (1 - h(x))  x^{(i)}\\
    % ### START CODE HERE ###
    % ### END CODE HERE ###
    \frac{\partial\log(1 - h_{\theta}(x^{(i)}))}{\partial\theta_k} &= 
    \frac{1}{1 - h(\theta^T x)} \times -1 \times h(x)(1 - h(x)) x_k\\
    &= - h_{\theta}(x) x^{(i)}
    % ### START CODE HERE ###
    % ### END CODE HERE ###
  \end{flalign*}

recalling that 
\begin{align*}
J(\theta) &= \sum_{i=1}^n 
   y^{(i)} log(h_{\theta} (x^{(i)})) + (1-y^{(i)})log(1 - h_{\theta} (x^{(i)})) 
\end{align*}
  Substituting into our equation for $J(\theta)$, we have
  %
      % ### START CODE HERE ###
  \begin{flalign*}
    \frac{\partial J(\theta)}{\partial\theta_k} &=
    \frac{1}{n} 
    \frac{ \partial \sum_{i=1}^n 
   y^{(i)} log(h_{\theta} (x^{(i)})) + (1-y^{(i)})log(1 - h_{\theta} (x^{(i)})) }
          {\partial \theta}\\
          &=     \frac{1}{n} 
          \sum_{i=1}^{n} {y^{(i)} (1 - h(x))  x^{(i)}} 
             + (1-y^{(i)}) \times - h_{\theta}(x) x^{(i)} \\
    &= \frac{1}{n} 
          \sum_{i=1}^{n}
          {y^{(i)} x^{(i)} - h(x) x^{(i)}  }   \\
    &= \frac{1}{n} 
          \sum_{i=1}^{n}
          {x^{(i)} (y^{(i)} - h(x) ) }            
 %   &=  \frac_{1}^{n} \sum_{i=1}^{n} {1}
  %  
    % ### END CODE HERE ###
  \end{flalign*}
  
  Consequently, the $(k, l)$ entry of the Hessian is given by
  
  \begin{flalign*}
    H_{kl} = \frac{\partial^2 J(\theta)}{\partial\theta_k\partial\theta_l} &=\\
    % ### START CODE HERE ###
    % ### END CODE HERE ###
  \end{flalign*}
  
  Using the fact that $X_{ij} = x_i x_j$ if and only if $X = xx^T$, we have
  
  \begin{flalign*}
    H &= \\
    % ### START CODE HERE ###
    % ### END CODE HERE ###
  \end{flalign*}

  To prove that $H$ is positive semi-definite, show $z^T Hz \ge 0$ for all $z\in\Re^\di$.
  
  \begin{flalign*}
    z^T H z &=\\
    % ### START CODE HERE ###
    % ### END CODE HERE ###
  \end{flalign*}
  
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1a
\clearpage

\LARGE
1.c
\normalsize

% <SCPD_SUBMISSION_TAG>_1c
\begin{answer}
  For shorthand, we let $\mc{H} = \{\phi, \Sigma, \mu_{0}, \mu_1\}$ denote
  the parameters for the problem.
  Since the given formulae are conditioned on $y$, use Bayes rule to get:
  \begin{align*}
    p(y =1\vert  x ; \mc{H}) &= \frac {p(x\vert y=1; \mc{H}) p(y=1; \mc{H})} {p(x; \mc{H})}\\
    & = \frac {p(x\vert y=1; \mc{H}) p(y=1; \mc{H})}
      {p(x\vert y=1; \mc{H}) p(y=1; \mc{H}) + p(x\vert y={0}; \mc{H}) p(y={0};
      \mc{H})}\\
    % ### START CODE HERE ###
   \end{align*}
   First note that
   \begin{align*}
   \frac{A}{A+B} &= \frac{1}{\frac{B+1}{A}}\\
   &= \frac{1}{1+\frac{B}{A}}
   \end{align*}
   Now letting $A=p(x\vert y=1; \mc{H}) p(y=1; \mc{H})$\\
   and $B=p(x\vert y=0; \mc{H}) p(y=0; \mc{H})$\\
   We can continue as
   \begin{align*}
        & = \frac {1}
        {1 + \frac{p(x\vert y=0; \mc{H}) p(y={0}; \mc{H})}
                  {p(x\vert y=1;   \mc{H}) p(y={1}; \mc{H})}}
    \end{align*}
    Noting that $p(y={1}; \mc{H})=\phi $ and  $p(y={0}; \mc{H}) = 1-\phi$
       \begin{align*}
        & = \frac {1}
        {1 + \frac{p(x\vert y=0; \mc{H}) (1-\phi)}
                  {p(x\vert y=1;   \mc{H}) \phi }}
    \end{align*}
    and noting that 
    \begin{align*}    
     p(x\vert y=i) &= \frac{1}{(2 \pi)^{\frac{d}{2}} \abs{\Sigma}^{\frac{1}{2}})} exp(-\frac{1}{2} (x- \mu_i)^{T} \Sigma^{-1} (x-\mu_i)) \\
     \end{align*}
     the $\frac{1}{(2 \pi)^{\frac{d}{2}}} $ terms will cancel leaving
     
   \begin{align*}
        & = \frac {1}
        {1 + \frac{exp(-\frac{1}{2} (x- \mu_0)^{T} \Sigma^{-1} (x-\mu_0))  (1-\phi)}
                  {exp(-\frac{1}{2} (x- \mu_1)^{T} \Sigma^{-1} (x-\mu_1))  \phi }}\\
  & = \frac {1}
   {1 + exp(-\frac{1}{2} (x- \mu_0)^{T} \Sigma^{-1} (x-\mu_0))  (1-\phi) + exp(-\frac{1}{2} (x- \mu_1)^{T} \Sigma^{-1} (x-\mu_1))  \phi }
    \end{align*}
     
    % ### END CODE HERE ###

\end{answer}
% <SCPD_SUBMISSION_TAG>_1c
\clearpage

\LARGE
1.d
\normalsize

% <SCPD_SUBMISSION_TAG>_1d
\begin{answer}
  First, derive the expression for the log-likelihood of the training data:
 \begin{flalign}
    \ell(\phi, \mu_{0}, \mu_1, \Sigma) &= \log \prod_{i=1}^\nexp p(x^{(i)} \vert  y^{(i)}; \mu_{0}, \mu_1, \Sigma) p(y^{(i)}; \phi)\\
    &= \sum_{i=1}^{\nexp} \log p(x^{(i)} \vert  y^{(i)}; \mu_{0}, \mu_1, \Sigma) +
    \sum_{i=1}^{n} \log p(y^{(i)}; \phi)\\
    % ### START CODE HERE ###
 &=  \sum_{i=1}^{\nexp} \log 
      \frac{1}{(2 \pi)^{\frac{d}{2}} \abs{\Sigma}^{\frac{1}{2}})}     
       exp(-\frac{1}{2} (x^{(i)} - 
        \mu_i)^{T} \Sigma^{-1} (x^{(i)} -\mu_i))  +
    \sum_{i=1}^{n} \log p(y^{(i)}; \phi)\\
 &=  \sum_{i=1}^{\nexp} \log 
      \frac{1}{(2 \pi)^{\frac{d}{2}} \abs{\Sigma}^{\frac{1}{2}})}
       exp(-\frac{1}{2} (x^{(i)} - \mu_i)^{T} \Sigma^{-1} (x^{(i)}-\mu_i))  +
    \sum_{i=1}^{n} \log \phi^{y^{(i)}} (1-\phi)^{(1-y^{(i)})}
    % ### END CODE HERE ###
  \end{flalign}

  Now, the likelihood is maximized by setting the derivative (or gradient) with respect to each of the parameters to zero.\\

  \textbf{For $\mathbf{\phi}$:}\\
Let $n_i$ be the number of $y$ values equal to $i$, for $i \in {0,1}$
  \begin{flalign*}
    \frac{\partial \ell}{\partial \phi}
    % ### START CODE HERE ###
    &= \frac{n_1 \partial log(\phi)}{\partial \phi} + 
     \frac{n_0 \partial log(1 - \phi)}{\partial \phi} \\
    &= \frac{n_1 }{n \phi} - 
     \frac{n_0}{ n (1 - \phi)}    \\ 
         &= \frac{n_1 (1-\phi) }{n \phi (1-\phi)} - 
     \frac{n_0 \phi}{n \phi (1 - \phi)} \\
&= \frac{n_1 }{n \phi} - 
     \frac{n_0}{ n (1 - \phi)}    \\ 
&= \frac{n_1 (1-\phi) - n_0 \phi}{n \phi (1 - \phi)}   
  \end{flalign*}
  Assuming $\phi$ is not $0$, then this is zero when
  \begin{align*}
  n_1 (1-\phi) &=  n_0 \phi \\
  n_1 - n_1 \phi &= n_0 \phi \\
  n_1 &= \phi (n_1 + n_0) \\
  \frac{n_1}{n_1 + n_0} &= \phi \\
  n_1 = \phi
  \end{align*}
    
    % ### END CODE HERE ###


  Setting this equal to zero and solving for $\phi$ gives the maximum
  likelihood estimate. \\

  \textbf{For $\mathbf{\mu_0}$:}

  {\bf Hint:}  Remember that $\Sigma$ (and thus $\Sigma^{-1}$) is symmetric.

  \begin{flalign*}
    \nabla_{\mu_{0}}\ell 
    % ### START CODE HERE ###
    &= \nabla_{\mu_0}  \sum_{i=1}^{\nexp} \log 
      \frac{1}{(2 \pi)^{\frac{d}{2}} \abs{\Sigma}^{\frac{1}{2}})}
       exp(-\frac{1}{2} (x^{(i)} - \mu_i)^{T} \Sigma^{-1} (x^{(i)}-\mu_i))  +
    \sum_{i=1}^{n} \log \phi^{y^{(i)}} (1-\phi)^{(1-y^{(i)})}\\
 &=  \nabla_{\mu_0}  \sum_{i=1}^{\nexp} \log 
      \frac{1}{(2 \pi)^{\frac{d}{2}} \abs{\Sigma}^{\frac{1}{2}})}
       exp(-\frac{1}{2} (x^{(i)} - \mu_i)^{T} \Sigma^{-1} (x^{(i)}-\mu_i)) \\
 &= {K} \nabla_{\mu_0}  \sum_{i=1}^{\nexp} 
     -\frac{1}{2} (x^{(i)} - \mu_0)^{T} \Sigma^{-1} (x^{(i)}-\mu_0)) \text{  for the $y^{(i)}=0$ and some constant $K$}     
       \end{flalign*}
 Now my matrix calculus is rusty but I know we will get terms that look like
 \begin{align*}
 &=  \nabla_{\mu_0}  \sum_{i=1}^{\nexp} \frac{1}{2} x^2 - 2 x^{(i)} \mu_0 + \mu_0^2 \\
\end{align*}    
and taking the gradient wrt $\mu_0$ we will get
\begin{align*}
 &=    \sum_{i=1}^{\nexp} - x^{(i)} + \mu_0 \text{ for the $y{(i)}=0$}\\
\end{align*}
and setting this to 0 will yield something like
  \begin{align*}
  \mu_0 &= \frac{\sum {\text{$x^{(i)}$ where $y^{(i)}=0$}}}{n_0}
  \end{align*}
  where $n_0$ is the number of $y^{(i)}=0$
    % ### END CODE HERE ###


  Setting this gradient to zero gives the maximum likelihood estimate
  for $\mu_{0}$.\\

  \textbf{For $\mathbf{\mu_1}$:}

  {\bf Hint:}  Remember that $\Sigma$ (and thus $\Sigma^{-1}$) is symmetric.
Similar to above
    %\nabla_{\mu_{1}}\ell 

    % ### START CODE HERE ###
      \begin{align*}
  \mu_1 &= \frac{\sum {\text{$x^{(i)}$ where $y^{(i)}=1$}}}{n_1}
  \end{align*}    
  where $n_1$ is the number of $y^{(i)}=1$  
    % ### END CODE HERE ###


  Setting this gradient to zero gives the maximum likelihood estimate
  for $\mu_{1}$.\\

  For $\Sigma$, we find the gradient with respect to $S = \Sigma^{-1}$ rather than $\Sigma$ just to simplify the derivation (note that $\vert S\vert  = \frac{1}{\vert \Sigma\vert }$).
  You should convince yourself that the maximum likelihood estimate $S_\nexp$ found in this way would correspond to the actual maximum likelihood estimate $\Sigma_\nexp$ as $S_\nexp^{-1} = \Sigma_\nexp$.

  {\bf Hint:}  You may need the following identities: 
  \begin{equation*}
    \nabla_S \vert S\vert  = \vert S\vert  (S^{-1})^T
  \end{equation*}
  \begin{equation*}
    \nabla_S b_i^T S b_i = \nabla_S tr \left( b_i^T S b_i \right) =
    \nabla_S tr \left( S b_i b_i^T \right) = b_i b_i^T
  \end{equation*}

  \begin{flalign*}
    \nabla_S\ell &=\\
    % ### START CODE HERE ###
  % ### END CODE HERE ###
  \end{flalign*}

  Next, substitute $\Sigma = S^{-1}$.  Setting this gradient to zero gives the required maximum likelihood estimate for $\Sigma$.\\
\end{answer}
% <SCPD_SUBMISSION_TAG>_1d
\clearpage

\LARGE
1.f
\normalsize

% <SCPD_SUBMISSION_TAG>_1f
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1f
\clearpage

\LARGE
1.g
\normalsize

% <SCPD_SUBMISSION_TAG>_1g
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1g
\clearpage

\LARGE
1.h
\normalsize

% <SCPD_SUBMISSION_TAG>_1h
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1h
\clearpage

\LARGE
2.c
\normalsize

% <SCPD_SUBMISSION_TAG>_2c
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2c
\clearpage

\LARGE
2.d
\normalsize

% <SCPD_SUBMISSION_TAG>_2d
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2d
\clearpage

\LARGE
2.e
\normalsize

% <SCPD_SUBMISSION_TAG>_2e
\begin{answer}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2e
\clearpage

% <SCPD_SUBMISSION_TAG>_entire_submission

\end{document}
